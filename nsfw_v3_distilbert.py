# -*- coding: utf-8 -*-
"""NSFW_v3_distilbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WDrymzbntDNrHYUvIjlfjSDHKplH2ZZz
"""

pip install transformers

pip install focal-loss

from google.colab import drive
drive.mount('/content/drive/')

import transformers

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split

import numpy as np

df = pd.read_excel("/content/drive/MyDrive/spam_consolidated.xlsx")

df.shape

df = df.dropna()

df.shape

df

df["label"].value_counts()

df1 = df[df["label"]=="SFW"]

df1

df_final = pd.concat([df1[:100000],df[df["label"]=="NSFW"]], axis=0)

dic = {"NSFW":1, "SFW":0}
df_final["label"] = df_final["label"].map(dic)

df_final

df_final["label"].value_counts()

df_final = df_final.dropna()

df_final.isnull().sum()

from transformers import DistilBertTokenizerFast

# Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)
MAX_LENGTH = 128


# Define function to encode text data in batches
def batch_encode(tokenizer, texts, batch_size=256, max_length=MAX_LENGTH):

    input_ids = []
    attention_mask = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        inputs = tokenizer.batch_encode_plus(batch,
                                             max_length=max_length,
                                             padding='longest', #implements dynamic padding
                                             truncation=True,
                                             return_attention_mask=True,
                                             return_token_type_ids=False
                                             )
        input_ids.extend(inputs['input_ids'])
        attention_mask.extend(inputs['attention_mask'])


    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)

X= df_final['text']
y=df_final['label']

# using the train test split function
X_train, X_test, y_train, y_test = train_test_split(X,y ,
                                   random_state=104,
                                   test_size=0.25,
                                   shuffle=True)

type(X_train)

X_train = X_train.astype(str)
X_test = X_test.astype(str)

# Encode X_train
X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())

# # Encode X_valid
# X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())

# Encode X_test
X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())

X_train_ids.shape

X_train_attention.shape

from transformers import TFDistilBertModel, DistilBertConfig

DISTILBERT_DROPOUT = 0.2
DISTILBERT_ATT_DROPOUT = 0.2

# Configure DistilBERT's initialization
config = DistilBertConfig(dropout=DISTILBERT_DROPOUT,
                          attention_dropout=DISTILBERT_ATT_DROPOUT,
                          output_hidden_states=True)

# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states
# and without any specific head on top.
distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)

# Make DistilBERT layers untrainable
for layer in distilBERT.layers:
    layer.trainable = False

import focal_loss
from focal_loss import BinaryFocalLoss

MAX_LENGTH = 128
LAYER_DROPOUT = 0.2
LEARNING_RATE = 5e-5
RANDOM_STATE = 42

def build_model(transformer, max_length=MAX_LENGTH):

    # Define weight initializer with a random seed to ensure reproducibility
    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE)

    # Define input layers
    input_ids_layer = tf.keras.layers.Input(shape=(max_length,),
                                            name='input_ids',
                                            dtype='int32')
    input_attention_layer = tf.keras.layers.Input(shape=(max_length,),
                                                  name='input_attention',
                                                  dtype='int32')

    # DistilBERT outputs a tuple where the first element at index 0
    # represents the hidden-state at the output of the model's last layer.
    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).
    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]

    # We only care about DistilBERT's output for the [CLS] token,
    # which is located at index 0 of every encoded sequence.
    # Splicing out the [CLS] tokens gives us 2D data.
    cls_token = last_hidden_state[:, 0, :]

    ##                                                 ##
    ## Define additional dropout and dense layers here ##
    ##                                                 ##

    # Define a single node that makes up the output layer (for binary classification)
    output = tf.keras.layers.Dense(1,
                                   activation='sigmoid',
                                   kernel_initializer=weight_initializer,
                                   kernel_constraint=None,
                                   bias_initializer='zeros'
                                   )(cls_token)

    # Define the model
    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)

    # Compile the model
    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE),
                  loss=BinaryFocalLoss(gamma=2),
                  metrics=['accuracy'])

    return model

y_train

X_train_ids

EPOCHS = 6
BATCH_SIZE = 64
NUM_STEPS = len(X_train.index) // BATCH_SIZE

model = build_model(distilBERT)

# Train the model
train_history1 = model.fit(
    x = [X_train_ids, X_train_attention],
    y = y_train.to_numpy(),
    epochs = EPOCHS,
    batch_size = BATCH_SIZE,
    steps_per_epoch = NUM_STEPS,

    verbose=2
)

model.save('distilbert_unfined.h5')

new_model = tf.keras.models.load_model('distilbert_unfined.h5', custom_objects={"TFDistilBertModel": transformers.TFDistilBertModel})

new_model.summary()

# Evaluate the restored model
loss, acc = new_model.evaluate(x = [X_train_ids, X_train_attention], y = y_train.to_numpy(), verbose=2)
print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))

X_test_ids

# Evaluate the restored model
loss, acc = new_model.evaluate(x = [X_test_ids, X_test_attention], y = y_test.to_numpy(), verbose=2)
print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))

inp_id, inp_att = tokenizer("you are a stupid")
new_model.predict(x = [X_test_ids, X_test_attention])

from sklearn import metrics
import matplotlib.pyplot as plt

new_model.metrics

from sklearn.metrics import classification_report

target_names = ['SFW', 'NSFW']
print(classification_report(y_train, y_pred, target_names=target_names))

# preds = new_model.predict([X_train_ids, X_train_attention])
pred_labels = np.argmax(preds.predictions, axis=1)
conf_matrix = metrics.confusion_matrix(y_train,pred_labels)
print('conf_matrix ',conf_matrix)

fig, ax = plt.subplots(figsize=(7.5, 7.5))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix(without preprocessing)', fontsize=18)
plt.show()

import torch
import torch.nn.functional as F
from sklearn import metrics

y_preds = []
y_trues = []
for index,val_text in enumerate(val_texts):
     tokenized_val_text = tokenizer([val_text],
                                    truncation=True,
                                    padding=True,
                                    return_tensor='pt')
     logits = model(tokenized_val_text)
     prediction = F.softmax(logits, dim=1)
     y_pred = torch.argmax(prediction).numpy()
     y_true = val_labels[index]
     y_preds.append(y_pred)
     y_trues.append(y_true)